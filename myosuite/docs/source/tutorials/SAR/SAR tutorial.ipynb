{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3084a00",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 42px;\">SAR tutorial</h1>\n",
    "\n",
    "In this notebook, we introduce synergistic action representations (SAR) and demonstrate how to use these representations to facilitate the training of high-dimensional continuous control policies within MyoSuite.\n",
    "\n",
    "The primary purpose of this notebook is to equip the user to train with SAR for their own task specification(s) both within and beyond the musculoskeletal control paradigm.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<h1 style=\"font-size: 28px;\">Contents of this notebook</h1>\n",
    "\n",
    "<ul style=\"line-height: 2; font-size: 16px;\">\n",
    "  <li><b>Imports and utilities</b>: set up tutorial with relevant imports and starter functions.\n",
    "  </li>\n",
    "  <li><b>Walkthrough of SAR core functions</b>: steps through and defines each of the four basic functions of the SAR method.\n",
    "  </li>\n",
    "  <li><b>Example 1: SAR x physiological locomotion</b>: end-to-end training example using the SAR method to learn forward locomotion on a diverse set of terrains.\n",
    "  </li>\n",
    "  <li><b>Example 2: SAR x physiological dexterity</b>: end-to-end training example using the SAR method to learn multiobject manipulation of parametric geometries.\n",
    "  </li>\n",
    "</ul>\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b5eddd",
   "metadata": {},
   "source": [
    "# Imports and utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6fe8e7",
   "metadata": {},
   "source": [
    "First, we import our required and helper functions from `SAR_tutorial_utils.py`. If you encounter import errors, it is recommended to `pip install` any imports that are missing in your particular environment by uncommenting out the appropriate lines below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8526f610",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip install stable-baselines3==1.7.0\n",
    "# !pip install joblib\n",
    "# !pip install scikit-learn\n",
    "# !pip install tqdm\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60114185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyoSuite:> Registering Myo Envs\n"
     ]
    }
   ],
   "source": [
    "# imports for SAR\n",
    "from SAR_tutorial_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc299ac0",
   "metadata": {},
   "source": [
    "# SAR core functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8433315",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: flex-start;\">\n",
    "    <div style=\"flex: 1;\">\n",
    "         Next, implement for SAR step by step. The framework follows the basic structure displayed below. Accordingly, we will implement functions that do each of the following:\n",
    "        <br>\n",
    "        <br>\n",
    "        <ol>\n",
    "            <li>Train a play phase policy.</li>\n",
    "            <li>Roll out play phase policy to capture muscle activations over time</li>\n",
    "            <li>Compute SAR from this activation dataset</li>\n",
    "            <li>Use computed SAR to train on target task</li>\n",
    "        </ol>\n",
    "        <br>\n",
    "        As we implement each of the above, we will also go into more detail about best practices for that step.\n",
    "        <br>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d8cadd",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;\">\n",
    "    <img src=\"./SAR_images/framework.png\" alt=\"SAR framework\" style=\"width:1200px; display:block; margin-left:auto; margin-right:auto;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c125c47",
   "metadata": {},
   "source": [
    "### Step 1: Train a play phase policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24053a43",
   "metadata": {},
   "source": [
    "When selecting a play phase environment, it is advised to select a task that is both (a) simple enough that vanilla RL will be able to make reasonable progress on the task, and (b) sufficiently similar to the target task such that a synergistic representation learned from this simpler task will actually be informative for the target behavior. For example, in our manipulation pipeline, the play phase is a reorientation task composed of a small number of geometries, while the target reorientation task is composed of a much larger set of geometries. It is helpful to think of the play phase as a kind of curriculum learning that 'eases' the agent into learning useful representations for the target task.\n",
    "\n",
    "`train()` enables the simple training of a policy using the stable-baselines3 implementation of SAC on a desired environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a94b385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env_name, policy_name, timesteps, seed):\n",
    "    \"\"\"\n",
    "    Trains a policy using sb3 implementation of SAC.\n",
    "    \n",
    "    env_name: str; name of gym env.\n",
    "    policy_name: str; choose unique identifier of this policy\n",
    "    timesteps: int; how long you want to train your policy for\n",
    "    seed: str (not int); relevant if you want to train multiple policies with the same params\n",
    "    \"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    env = Monitor(env)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecNormalize(env, norm_obs=True, norm_reward=False, clip_obs=10.)\n",
    "    \n",
    "    net_shape = [400, 300]\n",
    "    policy_kwargs = dict(net_arch=dict(pi=net_shape, qf=net_shape))\n",
    "    \n",
    "    model = SAC('MlpPolicy', env, learning_rate=linear_schedule(.001), buffer_size=int(3e5),\n",
    "            learning_starts=1000, batch_size=256, tau=.02, gamma=.98, train_freq=(1, \"episode\"),\n",
    "            gradient_steps=-1,policy_kwargs=policy_kwargs, verbose=1)\n",
    "    \n",
    "    succ_callback = SaveSuccesses(check_freq=1, env_name=env_name+'_'+seed, \n",
    "                             log_dir=f'{policy_name}_successes_{env_name}_{seed}')\n",
    "    \n",
    "    model.set_logger(configure(f'{policy_name}_results_{env_name}_{seed}'))\n",
    "    model.learn(total_timesteps=int(timesteps), callback=succ_callback, log_interval=4)\n",
    "    model.save(f\"{policy_name}_model_{env_name}_{seed}\")\n",
    "    env.save(f'{policy_name}_env_{env_name}_{seed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec30fc82",
   "metadata": {},
   "source": [
    "### Step 2: Roll out play phase policy to capture muscle activations over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a350c00b",
   "metadata": {},
   "source": [
    "`get_activations()` extracts muscle activation data at each timestep of the trained play phase policy's rollouts. In this implementation, `get_activations()` captures muscle activations from a given rollout if the rewards from that rollout fall above a certain threshold, computed as a percentile from a set of preview rollouts. \n",
    "\n",
    "Here, we set the number of sample episodes to 2000 and the reward percentile cutoff at 80%, though these should be considered hyperparameters that may require finetuning for a specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3be60e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(name, env_name, seed, episodes=2000, percentile=80):\n",
    "    \"\"\"\n",
    "    Returns muscle activation data from N runs of a trained policy.\n",
    "\n",
    "    name: str; policy name (see train())\n",
    "    env_name: str; name of the gym environment\n",
    "    seed: str; seed of the trained policy\n",
    "    episodes: int; optional; how many rollouts?\n",
    "    percentile: int; optional; percentile to set the reward threshold for considering an episode as successful\n",
    "    \"\"\"\n",
    "    with gym.make(env_name) as env:\n",
    "        env.reset()\n",
    "\n",
    "        model = SAC.load(f'{name}_model_{env_name}_{seed}')\n",
    "        vec = VecNormalize.load(f'{name}_env_{env_name}_{seed}', DummyVecEnv([lambda: env]))\n",
    "\n",
    "        # Calculate the reward threshold from 100 preview episodes\n",
    "        preview_rewards = []\n",
    "        for _ in range(100):\n",
    "            env.reset()\n",
    "            rewards = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                o = env.get_obs()\n",
    "                o = vec.normalize_obs(o)\n",
    "                a, __ = model.predict(o, deterministic=False)\n",
    "                next_o, r, done, info = env.step(a)\n",
    "                rewards += r\n",
    "            preview_rewards.append(rewards)\n",
    "        reward_threshold = np.percentile(preview_rewards, percentile)\n",
    "\n",
    "        # Run the main episode loop\n",
    "        solved_acts = []\n",
    "        for _ in tqdm(range(episodes)):\n",
    "            env.reset()\n",
    "            rewards, acts = 0, []\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                o = env.get_obs()\n",
    "                o = vec.normalize_obs(o)\n",
    "                a, __ = model.predict(o, deterministic=False)\n",
    "                next_o, r, done, info = env.step(a)\n",
    "                acts.append(env.sim.data.act.copy())\n",
    "                rewards += r\n",
    "\n",
    "            if rewards > reward_threshold:\n",
    "                solved_acts.extend(acts)\n",
    "\n",
    "    return solved_acts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fc01ec",
   "metadata": {},
   "source": [
    "### Step 3: Compute SAR from the activation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70af3cdc",
   "metadata": {},
   "source": [
    "We use linear representations (i.e., PCA, ICA, normalization) to approximate SAR. These representations are based directly on the motor neuroscience literature of representing muscle synergies (c.f., [Tresch et al, 2006](https://journals.physiology.org/doi/epdf/10.1152/jn.00222.2005)). Such representations also have the advantages of being highly interpretable and efficient for use as control signals. We also found in practice that nonlinear representations (such as VAE decoder networks) did not lead to strong performance are were far less efficient for training.\n",
    "\n",
    "Before computing SAR, we first seek to understand how informative each individual muscle synergy is for explaining the initial muscle activation dataset. `find_synergies` returns a dictionary (and optionally, a plot) that shows variance accounted for (VAF) by N synergies from the original muscle activation data. In general, we find that using a number of synergies where VAF > 80% leads to good performance, though this value should be treated as a hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "083dc8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_synergies(acts, plot=True):\n",
    "    \"\"\"\n",
    "    Computed % variance explained in the original muscle activation data with N synergies.\n",
    "    \n",
    "    acts: np.array; rollout data containing the muscle activations\n",
    "    plot: bool; whether to plot the result\n",
    "    \"\"\"\n",
    "    syn_dict = {}\n",
    "    for i in range(acts.shape[1]):\n",
    "        pca = PCA(n_components=i+1)\n",
    "        _ = pca.fit_transform(acts)\n",
    "        syn_dict[i+1] =  round(sum(pca.explained_variance_ratio_), 4)\n",
    "        \n",
    "    if plot:\n",
    "        plt.plot(list(syn_dict.keys()), list(syn_dict.values()))\n",
    "        plt.title('VAF by N synergies')\n",
    "        plt.xlabel('# synergies')\n",
    "        plt.ylabel('VAF')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "    return syn_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13361813",
   "metadata": {},
   "source": [
    "Once the number of synergies to use is determined (above), `compute_SAR()` will take the activation data as input and yield the synergistic action representation (SAR) in the form of ICA, PCA, and normalizer objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b8da034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_SAR(acts, n_syn, save=False):\n",
    "    \"\"\"\n",
    "    Takes activation data and desired n_comp as input and returns/optionally saves the ICA, PCA, and Scaler objects\n",
    "    \n",
    "    acts: np.array; rollout data containing the muscle activations\n",
    "    n_comp: int; number of synergies to use\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=n_syn)\n",
    "    pca_act = pca.fit_transform(acts)\n",
    "    \n",
    "    ica = FastICA()\n",
    "    pcaica_act = ica.fit_transform(pca_act)\n",
    "    \n",
    "    normalizer = MinMaxScaler((-1, 1))    \n",
    "    normalizer.fit(pcaica_act)\n",
    "    \n",
    "    if save:\n",
    "        joblib.dump(ica, 'ica_demo.pkl') \n",
    "        joblib.dump(pca, 'pca_demo.pkl')\n",
    "        joblib.dump(normalizer, 'scaler_demo.pkl')\n",
    "    \n",
    "    return ica, pca, normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d60f8d5",
   "metadata": {},
   "source": [
    "### Step 4: Use computed SAR to train on target task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c987cc",
   "metadata": {},
   "source": [
    "In order to train with SAR, we initialize a `SynNoSynWrapper`, which implements the policy architecture presented below. \n",
    "\n",
    "The policy takes as input $o_t$ and outputs an $O+N$-dimensional action vector, $a_{t}^{O+N}$, where $O$ is the dimensionality of the original action manifold and $N$ is the dimensionality of the synergistic manifold. The first $N$ synergistic actions are passed through $SAR(a_{t}^{N})=a_{t}^{SAR}$ to recover the synergistic muscle activations. The subsequent $O$ task-specific activations are mixed with the task-general activations using a linear blend $\\varphi$ to recover the final action $a_t^*$ that steps the environment forward. Note that for the locomotion experiments presented next, it was actually found that purely relying on the synergistic, task-general pathway ($\\varphi=1$) was sufficient for learning robust locomotion (see end-to-end example later in the notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d499c17",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;\">\n",
    "    <img src=\"./SAR_images/pol_arch.png\" alt=\"Policy Architecture\" style=\"width:850px; display:block; margin-left:auto; margin-right:auto;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "997767da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynNoSynWrapper(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    gym.ActionWrapper that reformulates the action space as the combination of a task-general synergy space and a\n",
    "    task-specific orginal space, and uses this mix to step the environment in the original action space.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, ica, pca, scaler, phi):\n",
    "        super().__init__(env)\n",
    "        self.ica = ica\n",
    "        self.pca = pca\n",
    "        self.scaler = scaler\n",
    "        self.weight = phi\n",
    "        \n",
    "        self.syn_act_space = self.pca.components_.shape[0]\n",
    "        self.no_syn_act_space = env.action_space.shape[0]\n",
    "        self.full_act_space = self.syn_act_space + self.no_syn_act_space\n",
    "        \n",
    "        self.action_space = gym.spaces.Box(low=-1., high=1., shape=(self.full_act_space,),dtype=np.float32)\n",
    "    def action(self, act):\n",
    "        syn_action = act[:self.syn_act_space]\n",
    "        no_syn_action = act[self.syn_act_space:]\n",
    "        \n",
    "        syn_action = self.pca.inverse_transform(self.ica.inverse_transform(self.scaler.inverse_transform([syn_action])))[0]\n",
    "        final_action = self.weight * syn_action + (1 - self.weight) * no_syn_action\n",
    "        \n",
    "        return final_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714d80be",
   "metadata": {},
   "source": [
    "Additionally, we initialize a `SynergyWrapper`, which implements a variation of the above policy architecture that only leverages task-general synergistic activations (i.e., a more efficient implementation for handling the case where $\\varphi=1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0df901de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynergyWrapper(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    gym.ActionWrapper that reformulates the action space as the synergy space and inverse transforms\n",
    "    synergy-exploiting actions back into the original muscle activation space.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, ica, pca, phi):\n",
    "        super().__init__(env)\n",
    "        self.ica = ica\n",
    "        self.pca = pca\n",
    "        self.scaler = phi\n",
    "        \n",
    "        self.action_space = gym.spaces.Box(low=-1., high=1., shape=(self.pca.components_.shape[0],),dtype=np.float32)\n",
    "    \n",
    "    def action(self, act):\n",
    "        action = self.pca.inverse_transform(self.ica.inverse_transform(self.scaler.inverse_transform([act])))\n",
    "        return action[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0070abb8",
   "metadata": {},
   "source": [
    "We proceed to train a policy with this architecture using `SAR_RL`. We find that a blend weight $\\varphi$=0.66 between the synergistic (task-general) and nonsynergistic (task-specific) activations works best in practice, though this should also be considered a hyperparameter that may require optimization for specific use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71fbeba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SAR_RL(env_name, policy_name, timesteps, seed, ica, pca, normalizer, phi=.66, syn_nosyn=True):\n",
    "    \"\"\"\n",
    "    Trains a policy using sb3 implementation of SAC + SynNoSynWrapper.\n",
    "    \n",
    "    env_name: str; name of gym env.\n",
    "    policy_name: str; choose unique identifier of this policy\n",
    "    timesteps: int; how long you want to train your policy for\n",
    "    seed: str (not int); relevant if you want to train multiple policies with the same params\n",
    "    ica: the ICA object\n",
    "    pca: the PCA object\n",
    "    normalizer: the normalizer object\n",
    "    phi: float; blend parameter between synergistic and nonsynergistic activations\n",
    "    \"\"\"\n",
    "    if syn_nosyn:\n",
    "        env = SynNoSynWrapper(gym.make(env_name), ica, pca, normalizer, phi)\n",
    "    else:\n",
    "        env = SynergyWrapper(gym.make(env_name), ica, pca, normalizer)\n",
    "    env = Monitor(env)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecNormalize(env, norm_obs=True, norm_reward=False, clip_obs=10.)\n",
    "    net_shape = [400, 300]\n",
    "    policy_kwargs = dict(net_arch=dict(pi=net_shape, qf=net_shape))\n",
    "    \n",
    "    model = SAC('MlpPolicy', env, learning_rate=linear_schedule(.001), buffer_size=int(3e5),\n",
    "            learning_starts=5000, batch_size=256, tau=.02, gamma=.98, train_freq=(1, \"episode\"),\n",
    "            gradient_steps=-1,policy_kwargs=policy_kwargs, verbose=1)\n",
    "    \n",
    "    succ_callback = SaveSuccesses(check_freq=1, env_name=env_name+'_'+seed, \n",
    "                             log_dir=f'{policy_name}_successes_{env_name}_{seed}')\n",
    "    \n",
    "    model.set_logger(configure(f'{policy_name}_results_{env_name}_{seed}'))\n",
    "    model.learn(total_timesteps=int(timesteps), callback=succ_callback, log_interval=4)\n",
    "    model.save(f\"{policy_name}_model_{env_name}_{seed}\")\n",
    "    env.save(f'{policy_name}_env_{env_name}_{seed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6d9302",
   "metadata": {},
   "source": [
    "# Full training example 1: SAR x physiological locomotion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549c4882",
   "metadata": {},
   "source": [
    "Now that we have defined the core functions for implementing SAR, we will now train a locomotion policy using MyoLegs. In this example:\n",
    "- a policy is trained on straight flat walking task for 1.5M steps (`myoLegWalk-v0`)\n",
    "- after training, we collect muscle activation data from policy rollouts\n",
    "- Variance accounted for (VAF) by N synergies is computed. Here, we use 20 synergies from the 80-dimensional leg muscle activations\n",
    "- SAR is used to train on:\n",
    "    - a hilly terrain walking task (`myoLegHillyTerrainWalk-v0`)\n",
    "    - an uneven terrain walking task (`myoLegRoughTerrainWalk-v0`)\n",
    "    - a stair climbing task (`myoLegStairTerrainWalk-v0`)\n",
    "\n",
    "<strong>Note: we also provide the option of using pretrained policies/representations at some of the above steps. Run the code below depending on your preferences.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc17b86",
   "metadata": {},
   "source": [
    "# Step 1.1\n",
    "\n",
    "First, we acquire our synergistic action representation. We provide two options for this:\n",
    "\n",
    "<strong>Option A: Get SAR from scratch (i.e., train on `myoLegWalk-v0` as play period → get muscle activations → compute SAR).</strong>\n",
    "\n",
    "<strong>Option B: Use the precomputed SAR.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297de557",
   "metadata": {},
   "source": [
    "## Option 1.1.A: Get SAR from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e29fc68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m    MyoSuite: A contact-rich simulation suite for musculoskeletal motor control\n",
      "        Vittorio Caggiano, Huawei Wang, Guillaume Durandau, Massimo Sartori, Vikash Kumar\n",
      "        L4DC-2019 | https://sites.google.com/view/myosuite\n",
      "    \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/miniconda3/envs/muscle/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.render_mode to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.render_mode` for environment variables or `env.get_wrapper_attr('render_mode')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/shubham/miniconda3/envs/muscle/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:77: UserWarning: The `render_mode` attribute is not defined in your environment. It will be set to None.\n",
      "  warnings.warn(\"The `render_mode` attribute is not defined in your environment. It will be set to None.\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "VecNormalize only supports `gym.spaces.Box` and `gym.spaces.Dict` observation spaces, not Box(403,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/shubham/ai707/muscle_memory/myosuite/docs/source/tutorials/SAR/SAR tutorial.ipynb Cell 32\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224b616973742d63227d/home/shubham/ai707/muscle_memory/myosuite/docs/source/tutorials/SAR/SAR%20tutorial.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train(\u001b[39m'\u001b[39;49m\u001b[39mmyoLegWalk-v0\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mplay_period\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m1.5e6\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m0\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/home/shubham/ai707/muscle_memory/myosuite/docs/source/tutorials/SAR/SAR tutorial.ipynb Cell 32\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224b616973742d63227d/home/shubham/ai707/muscle_memory/myosuite/docs/source/tutorials/SAR/SAR%20tutorial.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m env \u001b[39m=\u001b[39m Monitor(env)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224b616973742d63227d/home/shubham/ai707/muscle_memory/myosuite/docs/source/tutorials/SAR/SAR%20tutorial.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m env \u001b[39m=\u001b[39m DummyVecEnv([\u001b[39mlambda\u001b[39;00m: env])\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224b616973742d63227d/home/shubham/ai707/muscle_memory/myosuite/docs/source/tutorials/SAR/SAR%20tutorial.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m env \u001b[39m=\u001b[39m VecNormalize(env, norm_obs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, norm_reward\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, clip_obs\u001b[39m=\u001b[39;49m\u001b[39m10.\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224b616973742d63227d/home/shubham/ai707/muscle_memory/myosuite/docs/source/tutorials/SAR/SAR%20tutorial.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m net_shape \u001b[39m=\u001b[39m [\u001b[39m400\u001b[39m, \u001b[39m300\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224b616973742d63227d/home/shubham/ai707/muscle_memory/myosuite/docs/source/tutorials/SAR/SAR%20tutorial.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m policy_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(net_arch\u001b[39m=\u001b[39m\u001b[39mdict\u001b[39m(pi\u001b[39m=\u001b[39mnet_shape, qf\u001b[39m=\u001b[39mnet_shape))\n",
      "File \u001b[0;32m~/miniconda3/envs/muscle/lib/python3.8/site-packages/stable_baselines3/common/vec_env/vec_normalize.py:54\u001b[0m, in \u001b[0;36mVecNormalize.__init__\u001b[0;34m(self, venv, training, norm_obs, norm_reward, clip_obs, clip_reward, gamma, epsilon, norm_obs_keys)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39m# Check observation spaces\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm_obs:\n\u001b[1;32m     53\u001b[0m     \u001b[39m# Note: mypy doesn't take into account the sanity checks, which lead to several type: ignore...\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanity_checks()\n\u001b[1;32m     56\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space, spaces\u001b[39m.\u001b[39mDict):\n\u001b[1;32m     57\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_spaces \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mspaces\n",
      "File \u001b[0;32m~/miniconda3/envs/muscle/lib/python3.8/site-packages/stable_baselines3/common/vec_env/vec_normalize.py:123\u001b[0m, in \u001b[0;36mVecNormalize._sanity_checks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`norm_obs_keys` param is applicable only with `gym.spaces.Dict` observation spaces\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    124\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mVecNormalize only supports `gym.spaces.Box` and `gym.spaces.Dict` observation spaces, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnot \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: VecNormalize only supports `gym.spaces.Box` and `gym.spaces.Dict` observation spaces, not Box(403,)"
     ]
    }
   ],
   "source": [
    "train('myoLegWalk-v0', 'play_period', 1.5e6, '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ffb8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_name = 'walk_play_period_video'\n",
    "get_vid(name='play_period', env_name='myoLegWalk-v0', seed='0', episodes=10, video_name=video_name)\n",
    "show_video(f\"{video_name}.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9358d350",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "muscle_data = get_activations(name='play_period', env_name='myoLegWalk-v0', seed='0', episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c93851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_dict = find_synergies(muscle_data, plot=True)\n",
    "print(\"VAF by N synergies:\", syn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2512d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "ica,pca,normalizer = compute_SAR(muscle_data, 20, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcc6df4",
   "metadata": {},
   "source": [
    "## Option 1.1.B: Use precomputed SAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d76eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ica,pca,normalizer = load_locomotion_SAR()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e199df",
   "metadata": {},
   "source": [
    "# Step 1.2: Train on unseen terrain with synergies (SAR-RL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c91b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly choose terrain to learn, or manually choose one\n",
    "new_terrain = np.random.choice(['Hilly', 'Stair', 'Rough'])\n",
    "# new_terrain = ...\n",
    "print(f'myoLeg{new_terrain}TerrainWalk-v0 selected for training.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e101f52b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SAR_RL(env_name=f'myoLeg{new_terrain}TerrainWalk-v0', policy_name='SAR-RL', timesteps=2.5e6, \n",
    "       seed='0', ica=ica, pca=pca, normalizer=normalizer, phi=.66, syn_nosyn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6f812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_name = 'walk_SAR-RL_video'\n",
    "\n",
    "get_vid(name='SAR-RL', env_name=f'myoLeg{new_terrain}TerrainWalk-v0', seed='0', episodes=5, video_name=video_name, \n",
    "        determ=False, pca=pca, ica=ica, normalizer=normalizer, phi=.66, is_sar=True, syn_nosyn=False)\n",
    "\n",
    "show_video(f\"{video_name}.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d10cb1e",
   "metadata": {},
   "source": [
    "# Step 1.3 (optional): Train on unseen terrain with end-to-end RL (RL-E2E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea2ad12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(f'myoLeg{new_terrain}TerrainWalk-v0', 'RL-E2E', 4e6, '0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16629ae9",
   "metadata": {},
   "source": [
    "# Step 2.4: Plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851ac0f3",
   "metadata": {},
   "source": [
    "For convenience, a function is defined to automatically plot the results of these runs (ensure the names and seeds are preserved)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9424151",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(experiment='locomotion', terrain=new_terrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cf790b",
   "metadata": {},
   "source": [
    "# Full training example 2: SAR x physiological manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5cbb1e",
   "metadata": {},
   "source": [
    "Now, we turn to using synergies for manipulation rather than locomotion. In this example:\n",
    "- a policy is trained on an eight-object reorientation task, `Reorient8-v0`.\n",
    "- after training, we collect muscle activation data from policy rollouts\n",
    "- variance accounted for (VAF) by N synergies is computed. Here, we select N where VAF > 0.8.\n",
    "- SAR is computed at this VAF threshold.\n",
    "- SAR is used to train on a 100-object reorientation task, `Reorient100-v0`.\n",
    "\n",
    "<strong>Note: we also provide the option of using pretrained policies/representations at some of the above steps. Run the code below depending on your preferences.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1963d71",
   "metadata": {},
   "source": [
    "# Step 2.1\n",
    "\n",
    "First, we acquire our synergistic action representation. We provide two options for this:\n",
    "\n",
    "<strong>Option A: Get SAR from scratch (i.e., train on `Reorient8` as play period → get muscle activations → compute SAR).</strong>\n",
    "\n",
    "<strong>Option B: Use the precomputed SAR.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c960e8",
   "metadata": {},
   "source": [
    "## Option 2.1.A: Get SAR from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79831edb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m    SAR: Generalization of Physiological Agility and Dexterity via Synergistic Action Representation\n",
      "        Cameron Berg, Vittorio Caggiano*, Vikash Kumar*\n",
      "        RSS-2023 | https://sites.google.com/view/sar-rl/\n",
      "    \u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "VecNormalize only supports `gym.spaces.Box` and `gym.spaces.Dict` observation spaces, not Box(200,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/shubham/ai707/muscle_memory/myosuite/docs/source/tutorials/SAR/SAR tutorial.ipynb Cell 52\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224b616973742d63227d/home/shubham/ai707/muscle_memory/myosuite/docs/source/tutorials/SAR/SAR%20tutorial.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# a policy is trained on an eight-object reorientation task, Reorient8-v0\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224b616973742d63227d/home/shubham/ai707/muscle_memory/myosuite/docs/source/tutorials/SAR/SAR%20tutorial.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m train(env_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmyoHandReorient8-v0\u001b[39;49m\u001b[39m'\u001b[39;49m, policy_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mplay_period\u001b[39;49m\u001b[39m'\u001b[39;49m, timesteps\u001b[39m=\u001b[39;49m\u001b[39m1e6\u001b[39;49m, seed\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m0\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/home/shubham/ai707/muscle_memory/myosuite/docs/source/tutorials/SAR/SAR tutorial.ipynb Cell 52\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224b616973742d63227d/home/shubham/ai707/muscle_memory/myosuite/docs/source/tutorials/SAR/SAR%20tutorial.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m env \u001b[39m=\u001b[39m Monitor(env)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224b616973742d63227d/home/shubham/ai707/muscle_memory/myosuite/docs/source/tutorials/SAR/SAR%20tutorial.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m env \u001b[39m=\u001b[39m DummyVecEnv([\u001b[39mlambda\u001b[39;00m: env])\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224b616973742d63227d/home/shubham/ai707/muscle_memory/myosuite/docs/source/tutorials/SAR/SAR%20tutorial.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m env \u001b[39m=\u001b[39m VecNormalize(env, norm_obs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, norm_reward\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, clip_obs\u001b[39m=\u001b[39;49m\u001b[39m10.\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224b616973742d63227d/home/shubham/ai707/muscle_memory/myosuite/docs/source/tutorials/SAR/SAR%20tutorial.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m net_shape \u001b[39m=\u001b[39m [\u001b[39m400\u001b[39m, \u001b[39m300\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224b616973742d63227d/home/shubham/ai707/muscle_memory/myosuite/docs/source/tutorials/SAR/SAR%20tutorial.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m policy_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(net_arch\u001b[39m=\u001b[39m\u001b[39mdict\u001b[39m(pi\u001b[39m=\u001b[39mnet_shape, qf\u001b[39m=\u001b[39mnet_shape))\n",
      "File \u001b[0;32m~/miniconda3/envs/muscle/lib/python3.8/site-packages/stable_baselines3/common/vec_env/vec_normalize.py:54\u001b[0m, in \u001b[0;36mVecNormalize.__init__\u001b[0;34m(self, venv, training, norm_obs, norm_reward, clip_obs, clip_reward, gamma, epsilon, norm_obs_keys)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39m# Check observation spaces\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm_obs:\n\u001b[1;32m     53\u001b[0m     \u001b[39m# Note: mypy doesn't take into account the sanity checks, which lead to several type: ignore...\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanity_checks()\n\u001b[1;32m     56\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space, spaces\u001b[39m.\u001b[39mDict):\n\u001b[1;32m     57\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_spaces \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mspaces\n",
      "File \u001b[0;32m~/miniconda3/envs/muscle/lib/python3.8/site-packages/stable_baselines3/common/vec_env/vec_normalize.py:123\u001b[0m, in \u001b[0;36mVecNormalize._sanity_checks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`norm_obs_keys` param is applicable only with `gym.spaces.Dict` observation spaces\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    124\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mVecNormalize only supports `gym.spaces.Box` and `gym.spaces.Dict` observation spaces, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnot \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: VecNormalize only supports `gym.spaces.Box` and `gym.spaces.Dict` observation spaces, not Box(200,)"
     ]
    }
   ],
   "source": [
    "# a policy is trained on an eight-object reorientation task, Reorient8-v0\n",
    "train(env_name='myoHandReorient8-v0', policy_name='play_period', timesteps=1e6, seed='0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f34571",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# after training, we collect muscle activation data from policy rollouts\n",
    "muscle_data = get_activations(name='play_period', env_name='myoHandReorient8-v0', seed='0', episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a109e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAF by N synergies is computed. Here, N is selected where VAF > 0.8\n",
    "syn_dict = find_synergies(muscle_data, plot=True)\n",
    "print(\"VAF by N synergies:\", syn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c47ab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make and load a video of the trained policy\n",
    "\n",
    "video_name = 'play_period_vid'\n",
    "get_vid(name='play_period', env_name='myoHandReorient8-v0', seed='0', episodes=10, video_name=video_name)\n",
    "show_video(f\"{video_name}.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "313f39a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'muscle_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/shubham/ai707/muscle_memory/myosuite/docs/source/tutorials/SAR/SAR tutorial.ipynb Cell 56\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224b616973742d63227d/home/shubham/ai707/muscle_memory/myosuite/docs/source/tutorials/SAR/SAR%20tutorial.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# SAR is computed at this VAF threshold\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224b616973742d63227d/home/shubham/ai707/muscle_memory/myosuite/docs/source/tutorials/SAR/SAR%20tutorial.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m ica,pca,normalizer \u001b[39m=\u001b[39m compute_SAR(muscle_data, \u001b[39m20\u001b[39m, save\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'muscle_data' is not defined"
     ]
    }
   ],
   "source": [
    "# SAR is computed at this VAF threshold\n",
    "ica,pca,normalizer = compute_SAR(muscle_data, 20, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709e6fd8",
   "metadata": {},
   "source": [
    "## Option 2.1.B: Use precomputed SAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "592deab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/miniconda3/envs/muscle/lib/python3.8/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator FastICA from version 1.1.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/shubham/miniconda3/envs/muscle/lib/python3.8/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator PCA from version 1.1.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/shubham/miniconda3/envs/muscle/lib/python3.8/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator MinMaxScaler from version 1.1.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL TO USE PRECOMPUTED SAR\n",
    "ica,pca,normalizer = load_manipulation_SAR()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be153658",
   "metadata": {},
   "source": [
    "# Step 2.2: Train on Reorient100 with synergies (SAR-RL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0aaf7de9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/shubham/ai707/muscle_memory/myosuite/docs/source/tutorials/SAR/SAR tutorial.ipynb Cell 60\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224b616973742d63227d/home/shubham/ai707/muscle_memory/myosuite/docs/source/tutorials/SAR/SAR%20tutorial.ipynb#Y113sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# SAR is used to train on a 100-object reorientation task, Reorient100-v0\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224b616973742d63227d/home/shubham/ai707/muscle_memory/myosuite/docs/source/tutorials/SAR/SAR%20tutorial.ipynb#Y113sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mCUDA_VISIBLE_DEVICES=5\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224b616973742d63227d/home/shubham/ai707/muscle_memory/myosuite/docs/source/tutorials/SAR/SAR%20tutorial.ipynb#Y113sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m SAR_RL(env_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmyoHandReorient100-v0\u001b[39;49m\u001b[39m'\u001b[39;49m, policy_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mSAR-RL\u001b[39;49m\u001b[39m'\u001b[39;49m, timesteps\u001b[39m=\u001b[39;49m\u001b[39m1.5e6\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224b616973742d63227d/home/shubham/ai707/muscle_memory/myosuite/docs/source/tutorials/SAR/SAR%20tutorial.ipynb#Y113sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m        seed\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m0\u001b[39;49m\u001b[39m'\u001b[39;49m, ica\u001b[39m=\u001b[39;49mica, pca\u001b[39m=\u001b[39;49mpca, normalizer\u001b[39m=\u001b[39;49mnormalizer, phi\u001b[39m=\u001b[39;49m\u001b[39m.66\u001b[39;49m)\n",
      "\u001b[1;32m/home/shubham/ai707/muscle_memory/myosuite/docs/source/tutorials/SAR/SAR tutorial.ipynb Cell 60\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224b616973742d63227d/home/shubham/ai707/muscle_memory/myosuite/docs/source/tutorials/SAR/SAR%20tutorial.ipynb#Y113sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m net_shape \u001b[39m=\u001b[39m [\u001b[39m400\u001b[39m, \u001b[39m300\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224b616973742d63227d/home/shubham/ai707/muscle_memory/myosuite/docs/source/tutorials/SAR/SAR%20tutorial.ipynb#Y113sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m policy_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(net_arch\u001b[39m=\u001b[39m\u001b[39mdict\u001b[39m(pi\u001b[39m=\u001b[39mnet_shape, qf\u001b[39m=\u001b[39mnet_shape))\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224b616973742d63227d/home/shubham/ai707/muscle_memory/myosuite/docs/source/tutorials/SAR/SAR%20tutorial.ipynb#Y113sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m model \u001b[39m=\u001b[39m SAC(\u001b[39m'\u001b[39;49m\u001b[39mMlpPolicy\u001b[39;49m\u001b[39m'\u001b[39;49m, env, learning_rate\u001b[39m=\u001b[39;49mlinear_schedule(\u001b[39m.001\u001b[39;49m), buffer_size\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(\u001b[39m3e5\u001b[39;49m),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224b616973742d63227d/home/shubham/ai707/muscle_memory/myosuite/docs/source/tutorials/SAR/SAR%20tutorial.ipynb#Y113sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m         learning_starts\u001b[39m=\u001b[39;49m\u001b[39m5000\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m256\u001b[39;49m, tau\u001b[39m=\u001b[39;49m\u001b[39m.02\u001b[39;49m, gamma\u001b[39m=\u001b[39;49m\u001b[39m.98\u001b[39;49m, train_freq\u001b[39m=\u001b[39;49m(\u001b[39m1\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mepisode\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224b616973742d63227d/home/shubham/ai707/muscle_memory/myosuite/docs/source/tutorials/SAR/SAR%20tutorial.ipynb#Y113sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m         gradient_steps\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,policy_kwargs\u001b[39m=\u001b[39;49mpolicy_kwargs, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224b616973742d63227d/home/shubham/ai707/muscle_memory/myosuite/docs/source/tutorials/SAR/SAR%20tutorial.ipynb#Y113sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m succ_callback \u001b[39m=\u001b[39m SaveSuccesses(check_freq\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, env_name\u001b[39m=\u001b[39menv_name\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mseed, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224b616973742d63227d/home/shubham/ai707/muscle_memory/myosuite/docs/source/tutorials/SAR/SAR%20tutorial.ipynb#Y113sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m                          log_dir\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpolicy_name\u001b[39m}\u001b[39;00m\u001b[39m_successes_\u001b[39m\u001b[39m{\u001b[39;00menv_name\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mseed\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224b616973742d63227d/home/shubham/ai707/muscle_memory/myosuite/docs/source/tutorials/SAR/SAR%20tutorial.ipynb#Y113sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m model\u001b[39m.\u001b[39mset_logger(configure(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpolicy_name\u001b[39m}\u001b[39;00m\u001b[39m_results_\u001b[39m\u001b[39m{\u001b[39;00menv_name\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mseed\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/muscle/lib/python3.8/site-packages/stable_baselines3/sac/sac.py:149\u001b[0m, in \u001b[0;36mSAC.__init__\u001b[0;34m(self, policy, env, learning_rate, buffer_size, learning_starts, batch_size, tau, gamma, train_freq, gradient_steps, action_noise, replay_buffer_class, replay_buffer_kwargs, optimize_memory_usage, ent_coef, target_update_interval, target_entropy, use_sde, sde_sample_freq, use_sde_at_warmup, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ment_coef_optimizer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[39mif\u001b[39;00m _init_setup_model:\n\u001b[0;32m--> 149\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_model()\n",
      "File \u001b[0;32m~/miniconda3/envs/muscle/lib/python3.8/site-packages/stable_baselines3/sac/sac.py:152\u001b[0m, in \u001b[0;36mSAC._setup_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_setup_model\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 152\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_setup_model()\n\u001b[1;32m    153\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_aliases()\n\u001b[1;32m    154\u001b[0m     \u001b[39m# Running mean and running var\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/muscle/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py:221\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm._setup_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplay_buffer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplay_buffer_class(\n\u001b[1;32m    206\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer_size,\n\u001b[1;32m    207\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplay_buffer_kwargs,\n\u001b[1;32m    213\u001b[0m     )\n\u001b[1;32m    215\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_class(  \u001b[39m# pytype:disable=not-instantiable\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space,\n\u001b[1;32m    217\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space,\n\u001b[1;32m    218\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_schedule,\n\u001b[1;32m    219\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_kwargs,  \u001b[39m# pytype:disable=not-instantiable\u001b[39;00m\n\u001b[1;32m    220\u001b[0m )\n\u001b[0;32m--> 221\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    223\u001b[0m \u001b[39m# Convert train freq parameter to TrainFreq object\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_train_freq()\n",
      "File \u001b[0;32m~/miniconda3/envs/muscle/lib/python3.8/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/miniconda3/envs/muscle/lib/python3.8/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/muscle/lib/python3.8/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/muscle/lib/python3.8/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/muscle/lib/python3.8/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/muscle/lib/python3.8/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# SAR is used to train on a 100-object reorientation task, Reorient100-v0\n",
    "SAR_RL(env_name='myoHandReorient100-v0', policy_name='SAR-RL', timesteps=1.5e6, \n",
    "       seed='0', ica=ica, pca=pca, normalizer=normalizer, phi=.66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba8a950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make and load a video of the trained policy\n",
    "\n",
    "video_name = 'SAR_vid'\n",
    "\n",
    "get_vid(name='SAR-RL', env_name='myoHandReorient100-v0', seed='0', episodes=10, video_name=video_name, \n",
    "        determ=True, pca=pca, ica=ica, normalizer=normalizer, phi=.66, is_sar=True, syn_nosyn=True)\n",
    "show_video(f\"{video_name}.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dc00d6",
   "metadata": {},
   "source": [
    "# Step 2.3 (optional): Train on Reorient100 with end-to-end RL (RL-E2E)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f27d1",
   "metadata": {},
   "source": [
    "We can compare the performance of (A) pretraining for 1M steps on `Reorient8-v0`, getting SAR, and training with SAR-RL on `Reorient100-v0` for 2M steps with (B) training for 3M steps using end-to-end RL ('RL-E2E')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d36ff7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(env_name='myoHandReorient100-v0', policy_name='RL-E2E', timesteps=2.5e6, seed='0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d350e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make and load a video of the trained policy\n",
    "\n",
    "video_name = 'RL+E2E_vid'\n",
    "get_vid('RL-E2E', 'myoHandReorient100-v0', '0', determ=False, episodes=10, video_name=video_name)\n",
    "show_video(f\"{video_name}.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3154c777",
   "metadata": {},
   "source": [
    "# Step 2.4: Plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1717e5",
   "metadata": {},
   "source": [
    "For convenience, a function is defined to automatically plot the results of these runs (ensure the names and seeds are preserved)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05215d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(experiment='manipulation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7bc976",
   "metadata": {},
   "source": [
    "# Step 2.5 (optional): zero-shot testing manipulation policies on new unseen objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bba92d",
   "metadata": {},
   "source": [
    "We can optionally test these manipulation policies on unseen objects to determine their generalizability. Here, we implement two test\n",
    "environments.\n",
    "\n",
    "1. ReorientID-v0—1000 unseen objects generated by sampling from the same dimensions as those for the Reorient100-v0 set. Intuitively, this environment contains new objects with the same kind of shapes as those seen in the training set.\n",
    "\n",
    "2. ReorientOOD-v0—1000 unseen objects generated by sampling from different dimensions as those for the Reorient100-v0 set. Intuitively, this environment contains new objects with different kinds of shapes as those seen in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba66889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroshot_test(name, test_env_name, env_name='myoHandReorient100-v0', seed='0', determ=True, ica=None, \n",
    "                  pca=None, normalizer=None, phi=.66, episodes=500, is_sar=False, syn_nosyn=False):\n",
    "    \"\"\"\n",
    "    Check zero-shot performance of policies on the test environments.\n",
    "    \n",
    "    name: str; name of the policy to test\n",
    "    env_name: str; name of gym env the policy to test was trained on (Reorient100-v0).\n",
    "    seed: str; seed of the policy to test\n",
    "    test_env_name: str; name of the desired test env\n",
    "    ica: if testing SAR-RL, the ICA object\n",
    "    pca: if testing SAR-RL, the PCA object\n",
    "    normalizer: if testing SAR-RL, the normalizer object\n",
    "    phi: float; blend parameter between synergistic and nonsynergistic activations\n",
    "    episodes: int; number of episodes to run on the test environment\n",
    "    \"\"\"\n",
    "    if is_sar:\n",
    "        if syn_nosyn:\n",
    "            env = SynNoSynWrapper(gym.make(test_env_name), ica, pca, normalizer, phi)\n",
    "        else:\n",
    "            env = SynergyWrapper(gym.make(test_env_name), ica, pca, normalizer, phi)\n",
    "    else:\n",
    "        env = gym.make(test_env_name)\n",
    "    env.reset()\n",
    "\n",
    "    model = SAC.load(f'{name}_model_{env_name}_{seed}')\n",
    "    vec = VecNormalize.load(f'{name}_env_{env_name}_{seed}', DummyVecEnv([lambda: env]))\n",
    "    solved = []\n",
    "    for i,_ in enumerate(range(episodes)):\n",
    "        is_solved = []\n",
    "        env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            o = env.get_obs()\n",
    "            o = vec.normalize_obs(o)\n",
    "            a, __ = model.predict(o, deterministic=determ)\n",
    "            next_o, r, done, info = env.step(a)\n",
    "            is_solved.append(info['solved'])\n",
    "        \n",
    "        if sum(is_solved) > 0:\n",
    "            solved.append(1)\n",
    "        else:\n",
    "            solved.append(0)\n",
    "\n",
    "    env.close()\n",
    "    return np.mean(solved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08578ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# zero-shot testing RL-E2E policy\n",
    "\n",
    "name = 'RL-E2E'\n",
    "\n",
    "e2e_id = zeroshot_test(name, 'myoHandReorientID-v0')\n",
    "\n",
    "e2e_ood = zeroshot_test(name, 'myoHandReorientOOD-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c97370",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# zero-shot testing SAR-RL policy\n",
    "\n",
    "name = 'SAR-RL'\n",
    "\n",
    "sar_id = zeroshot_test(name, 'myoHandReorientID-v0', ica=ica, pca=pca, \n",
    "                       normalizer=normalizer, is_sar=True, syn_nosyn=True)\n",
    "\n",
    "sar_ood = zeroshot_test(name, 'myoHandReorientOOD-v0', ica=ica, pca=pca, \n",
    "                        normalizer=normalizer, is_sar=True, syn_nosyn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7cfe1b",
   "metadata": {},
   "source": [
    "We can visualize these zero-shot generalization results by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18035c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "    \n",
    "zeroshot = {\"SAR-RL\": [sar_id,sar_ood],\n",
    "            \"RL-E2E\": [e2e_id,e2e_ood]}\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plot_zeroshot(ax, zeroshot)\n",
    "fig.set_size_inches(6, 5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
